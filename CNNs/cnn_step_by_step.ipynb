{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_step_by_step.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNxQgc43HOTC7KpQBOnkMx+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuralsrg/SupervisedLearning/blob/main/CNNs/cnn_step_by_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN | Manual Implementation"
      ],
      "metadata": {
        "id": "7y455769zQ5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random.seed(1)"
      ],
      "metadata": {
        "id": "_bE2RT5CzPt-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding"
      ],
      "metadata": {
        "id": "3ZLuFuLw9Nw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_pad(X, pad):\n",
        "  '''\n",
        "  Give the dataset tensor of the shape (n_samples, n_h, n_w, n_channels), pad\n",
        "  all images with a given pad.\n",
        "  \n",
        "  Argument:\n",
        "  X -- ndarray of shape e (n_samples, n_h, n_w, n_channels)\n",
        "  pad -- padding size\n",
        "\n",
        "  Returns:\n",
        "  X_pad -- padded dataset of shape\n",
        "    (n_samples, n_h + 2*pad, n_w + 2*pad, n_channels)\n",
        "  '''\n",
        "\n",
        "  return np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode='constant',\n",
        "                constant_values=(0, 0))\n",
        "  \n",
        "# testing\n",
        "\n",
        "x = np.random.rand(1, 3, 3, 1)\n",
        "x_pad = zero_pad(x, pad=3)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "\n",
        "axes[0].set_title('x')\n",
        "axes[0].imshow(x[0, :, :, 0])\n",
        "\n",
        "axes[1].set_title('x_pad')\n",
        "axes[1].imshow(x_pad[0, :, :, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "BiGOvzG4zaR-",
        "outputId": "a51e6bed-1ba2-45b3-d6de-6cf0712dbb92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f364c882b90>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward pass"
      ],
      "metadata": {
        "id": "zPV8lHHm9Ruf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_single_step(a, W, b):\n",
        "  '''\n",
        "  Applies convolution for a and filter defined by W\n",
        "\n",
        "  Arguments:\n",
        "  a -- slice of input data of shape (f, f, n_C_prev)\n",
        "  W -- weight matrix of shape (f, f, n_C_prev)\n",
        "  b -- bias patameter\n",
        "\n",
        "  Returns:\n",
        "  Z -- scalar value, the result of one-step convolution\n",
        "  '''\n",
        "\n",
        "  Z = np.sum(np.multiply(a, W))\n",
        "  return Z + np.squeeze(b)"
      ],
      "metadata": {
        "id": "avNaAPhw2a9j"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv(A_prev, W, b, params={'pad' : 0, 'stride' : 1}):\n",
        "  '''\n",
        "  Computes convolution of A_pred with filter defined by W\n",
        "\n",
        "  Arguments:\n",
        "  A_prev -- activation tensor from the previous layer of shape\n",
        "    (n_samples, n_h, n_w, n_channels)\n",
        "  W -- weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
        "  b -- biases, numpy array of shape (1, 1, 1, n_C)\n",
        "  params -- python dictionary containing 'pad' & 'stride' sizes\n",
        "\n",
        "  Returns: \n",
        "  Z -- convolution output \n",
        "  cache -- information for backprop\n",
        "  '''\n",
        "\n",
        "  (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "  (f, f, n_C_prev_check, n_C) = W.shape\n",
        "\n",
        "  assert n_C_prev == n_C_prev_check # must be equal\n",
        "\n",
        "  pad = params['pad']\n",
        "  stride = params['stride']\n",
        "  \n",
        "  n_H = np.floor((n_H_prev + 2*pad - f)/stride) + 1\n",
        "  n_W = np.floor((n_W_prev + 2*pad - f)/stride) + 1\n",
        "\n",
        "  # output volume \n",
        "  Z = np.zeros((m, n_H, n_W, n_C))\n",
        "\n",
        "  # Create A_prev_pad by padding A_prev\n",
        "  A_prev_pad = zero_pad(A_prev, pad)\n",
        "\n",
        "  for i in range(m):               # loop over the batch of training examples\n",
        "    a_prev_pad = A_prev_pad[i]     # Select i-th training example's padded activation\n",
        "\n",
        "    for h in range(n_H):           # loop over vertical axis of the output volume\n",
        "      # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
        "      vert_start = stride * h \n",
        "      vert_end = vert_start  + f\n",
        "      \n",
        "      for w in range(n_W):       # loop over horizontal axis of the output volume\n",
        "        # Find the horizontal start and end of the current \"slice\" (≈2 lines)\n",
        "        horiz_start = stride * w\n",
        "        horiz_end = horiz_start + f\n",
        "        \n",
        "        for c in range(n_C):   # loop over channels (= #filters) of the output volume\n",
        "                                \n",
        "          # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
        "          a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
        "          \n",
        "          # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈3 line)\n",
        "          weights = W[:, :, :, c]\n",
        "          biases  = b[:, :, :, c]\n",
        "          Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\n",
        "\n",
        "  # Save information in \"cache\" for the backprop\n",
        "  cache = (A_prev, W, b, params)\n",
        "\n",
        "  return Z, cache"
      ],
      "metadata": {
        "id": "Z7dIrhUS5pKh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling "
      ],
      "metadata": {
        "id": "zo7fEsdD9a4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pool_forward(A_prev, params, mode=\"max\"):\n",
        "  \"\"\"\n",
        "  Implements the forward pass of the pooling layer\n",
        "  \n",
        "  Arguments:\n",
        "  A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "  params -- python dictionary containing \"f\" and \"stride\"\n",
        "  mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
        "  \n",
        "  Returns:\n",
        "  A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
        "  cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
        "  \"\"\"\n",
        "  \n",
        "  (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "\n",
        "  f = params[\"f\"]\n",
        "  stride = params[\"stride\"]\n",
        "\n",
        "  n_H = np.floor(1 + (n_H_prev - f) / stride)\n",
        "  n_W = np.floor(1 + (n_W_prev - f) / stride)\n",
        "  n_C = n_C_prev\n",
        "  \n",
        "  A = np.zeros((m, n_H, n_W, n_C))              \n",
        "  \n",
        "  for i in range(m):                         # loop over the training examples\n",
        "    a_prev_slice = A_prev[i]\n",
        "\n",
        "    for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
        "      # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
        "      vert_start = stride * h \n",
        "      vert_end = vert_start + f\n",
        "      \n",
        "      for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
        "        # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
        "        horiz_start = stride * w\n",
        "        horiz_end = horiz_start + f\n",
        "        \n",
        "        for c in range (n_C):            # loop over the channels of the output volume\n",
        "          \n",
        "          # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
        "          a_slice_prev = a_prev_slice[vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "          \n",
        "          # Compute the pooling operation on the slice. \n",
        "          # Use an if statement to differentiate the modes. \n",
        "          # Use np.max and np.mean.\n",
        "          if mode == \"max\":\n",
        "            A[i, h, w, c] = np.max(a_slice_prev)\n",
        "          elif mode == \"average\":\n",
        "            A[i, h, w, c] = np.mean(a_slice_prev)\n",
        "          else:\n",
        "            print(mode+ \"-type pooling layer NOT Defined\")    \n",
        "  \n",
        "  # Store the input and hparameters in \"cache\" for pool_backward()\n",
        "  cache = (A_prev, params)\n",
        "  \n",
        "  # Making sure your output shape is correct\n",
        "  assert(A.shape == (m, n_H, n_W, n_C))\n",
        "  \n",
        "  return A, cache"
      ],
      "metadata": {
        "id": "oZxp_o6X9KYq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation"
      ],
      "metadata": {
        "id": "PzbJn4nwQiw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weights, biases\n",
        "\n",
        "Forward pass: $ Z_{HW} = [X_{HW} * W + b_C] $\n",
        "\n",
        "$ L = f(Z_{00}, ..., Z_{HW}); \\frac{\\partial{Z_{HW}}}{\\partial{b_C}} = 1 $\n",
        "\n",
        "For each channel: $ db_C = \\sum_{h}\\sum_{w}dZ_{HW} $"
      ],
      "metadata": {
        "id": "S4utY_kKI3kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_backward(dZ, cache):\n",
        "  '''\n",
        "  Backward propagation for a convolution function\n",
        "  \n",
        "  Arguments:\n",
        "  dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
        "  cache -- cache of values needed for the conv_backward(), output of conv()\n",
        "  \n",
        "  Returns:\n",
        "  dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
        "              numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "  dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
        "        numpy array of shape (f, f, n_C_prev, n_C)\n",
        "  db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
        "        numpy array of shape (1, 1, 1, n_C)\n",
        "  '''\n",
        "\n",
        "  (A_prev, W, b, params) = cache\n",
        "  (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "  (f, f, n_C_prev_check, n_C) = W.shape\n",
        "\n",
        "  assert n_C_prev == n_C_prev_check\n",
        "  \n",
        "  pad = params[\"pad\"]\n",
        "  stride = params[\"stride\"]\n",
        "  \n",
        "  (m, n_H, n_W, n_C) = dZ.shape\n",
        "  \n",
        "  # Initialize dA_prev, dW, db with the correct shapes\n",
        "  dA_prev = np.zeros(A_prev.shape)                          \n",
        "  dW = np.zeros(W.shape)\n",
        "  db = np.zeros(b.shape) # b.shape = [1,1,1,n_C]\n",
        "  \n",
        "  # Pad A_prev and dA_prev\n",
        "  A_prev_pad = zero_pad(A_prev, pad)\n",
        "  dA_prev_pad = zero_pad(dA_prev, pad)\n",
        "  \n",
        "  for i in range(m):                   # loop over the training examples\n",
        "      \n",
        "    # select ith training example from A_prev_pad and dA_prev_pad\n",
        "    a_prev_pad = A_prev_pad[i]\n",
        "    da_prev_pad = dA_prev_pad[i]\n",
        "    \n",
        "    for h in range(n_H):               # loop over vertical axis of the output volume\n",
        "      for w in range(n_W):             # loop over horizontal axis of the output volume\n",
        "        for c in range(n_C):           # loop over the channels of the output volume\n",
        "            \n",
        "          # Find the corners of the current \"slice\"\n",
        "          vert_start = stride * h \n",
        "          vert_end = vert_start + f\n",
        "          horiz_start = stride * w\n",
        "          horiz_end = horiz_start + f\n",
        "\n",
        "          # Use the corners to define the slice from a_prev_pad\n",
        "          a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
        "\n",
        "          # Update gradients for the window and the filter's parameters using the code formulas given above\n",
        "          da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
        "          dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
        "          db[:,:,:,c] += dZ[i, h, w, c]\n",
        "                \n",
        "    # Set the ith training example's dA_prev to the unpadded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
        "    dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
        "  \n",
        "  # Making sure your output shape is correct\n",
        "  assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
        "  \n",
        "  return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "AYfhhSsW-viz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pooling "
      ],
      "metadata": {
        "id": "oQOp-pD_Qj2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Max Pool"
      ],
      "metadata": {
        "id": "6ZOOBm6yQoM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# masks max element in the matrix\n",
        "\n",
        "def create_mask_from_window(x):\n",
        "\n",
        "  return x == np.max(x)"
      ],
      "metadata": {
        "id": "yGvZ7pHtQmve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Average Pool"
      ],
      "metadata": {
        "id": "3zddXqs8UxJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def distribute_value(dz, shape):\n",
        "  # Retrieve dimensions from shape (≈1 line)\n",
        "  (n_H, n_W) = shape\n",
        "  \n",
        "  # Compute the value to distribute on the matrix (≈1 line)\n",
        "  average = np.prod(shape)\n",
        "  \n",
        "  # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
        "  a = (dz/average)*np.ones(shape)\n",
        "\n",
        "  return a"
      ],
      "metadata": {
        "id": "BYO-zQutU0Ou"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pool_backward(dA, cache, mode = \"max\"):\n",
        "  \"\"\"\n",
        "  Implements the backward pass of the pooling layer\n",
        "  \n",
        "  Arguments:\n",
        "  dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
        "  cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
        "  mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
        "  \n",
        "  Returns:\n",
        "  dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
        "  \"\"\"\n",
        "  # Retrieve information from cache (≈1 line)\n",
        "  (A_prev, hparameters) = cache\n",
        "  \n",
        "  # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
        "  stride = hparameters[\"stride\"]\n",
        "  f = hparameters[\"f\"]\n",
        "  \n",
        "  # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
        "  m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "  m, n_H, n_W, n_C = dA.shape\n",
        "  \n",
        "  # Initialize dA_prev with zeros (≈1 line)\n",
        "  dA_prev = np.zeros(A_prev.shape)\n",
        "  \n",
        "  for i in range(m): # loop over the training examples\n",
        "      \n",
        "    # select training example from A_prev (≈1 line)\n",
        "    a_prev = A_prev[i,:,:,:]\n",
        "    \n",
        "    for h in range(n_H):               # loop on the vertical axis\n",
        "      for w in range(n_W):             # loop on the horizontal axis\n",
        "        for c in range(n_C):           # loop over the channels (depth)\n",
        "\n",
        "          # Find the corners of the current \"slice\" (≈4 lines)\n",
        "          vert_start  = h * stride\n",
        "          vert_end    = h * stride + f\n",
        "          horiz_start = w * stride\n",
        "          horiz_end   = w * stride + f\n",
        "          \n",
        "          # Compute the backward propagation in both modes.\n",
        "          if mode == \"max\":\n",
        "\n",
        "            a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "            \n",
        "            mask = create_mask_from_window( a_prev_slice )\n",
        "            dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
        "            \n",
        "          elif mode == \"average\":\n",
        "            \n",
        "            da = dA[i, h, w, c]\n",
        "            shape = (f, f)\n",
        "            dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
        "\n",
        "  # Making sure your output shape is correct\n",
        "  assert(dA_prev.shape == A_prev.shape)\n",
        "  \n",
        "  return dA_prev"
      ],
      "metadata": {
        "id": "3_JmWeUEU6Lt"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}